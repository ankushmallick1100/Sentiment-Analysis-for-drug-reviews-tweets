{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d6f72-7bbc-4778-911c-cad9f75f88a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings \n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac491823-79b1-4d21-8a82-54f6294b05ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Train dataset\n",
    "train = pd.read_csv('train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee40787-dc48-4f90-8f86-c66208950abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Test dataset\n",
    "test = pd.read_csv('test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77749e65-d2a0-4c0e-b28a-79e456a4fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Two Dataset - Train and Test Dataset\n",
    "data = pd.concat([train, test])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76d294f-16c2-4f95-9881-04f4a484d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c640707-62be-4edf-a83f-32eb53d55abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5a8667-3a83-4bc6-99b9-3d132b239d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4401403b-38d1-45e6-a19d-8a974d3b48fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c74663b-b727-46c1-8201-ca86735772dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column Review_Sentiment\n",
    "data.loc[(data['rating'] >= 5), 'Review_Sentiment'] = 1\n",
    "data.loc[(data['rating'] < 5), 'Review_Sentiment'] = 0\n",
    "\n",
    "data['Review_Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d91a1c-6479-4c0d-947e-ca83361a0c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the date into datetime format\n",
    "data['date'] = pd.to_datetime(data['date'], errors = 'coerce')\n",
    "\n",
    "# Extracting year, month, date from date\n",
    "data['Year'] = data['date'].dt.year\n",
    "data['Month'] = data['date'].dt.month\n",
    "data['Day'] = data['date'].dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62638dd-262e-4986-bcc7-49479c2ca065",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1167f9-2b3d-4aa3-99b4-077aaccb1535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_clean(review):\n",
    "    # Changing reviews to lower case\n",
    "    to_lower = review.str.lower()\n",
    "    \n",
    "    # Replacing the repeating pattern of &#039;\n",
    "    repeat_pattern_remove = to_lower.str.replace(\"&#039;\", \"\")\n",
    "    \n",
    "    # Removing all special characters\n",
    "    special_char_remove = repeat_pattern_remove.str.replace(r'[^\\w\\d\\s]',' ')\n",
    "    \n",
    "    # Removing all ASCII characters\n",
    "    non_ascii_char_remove = special_char_remove.str.replace(r'[^\\x00-\\x7F]+',' ')\n",
    "    \n",
    "    # Removing the leading and trailing whitespaces\n",
    "    whitespace_remove = non_ascii_char_remove.str.replace(r'^\\s+|\\s+?$','')\n",
    "    \n",
    "    # Replacing multiple spaces with single space\n",
    "    multiple_space_remove = whitespace_remove.str.replace(r'\\s+',' ')\n",
    "    \n",
    "    # Replacing two or more dots with one\n",
    "    dataframe = multiple_space_remove.str.replace(r'\\.{2,}', ' ')\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d7e61-0b9f-4373-b3a0-fc444afff3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['review_clean'] = review_clean(data['review'])\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f976495c-ad32-4e81-96d6-fe1991ec7112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contraction Dictionary for the expansion\n",
    "import re\n",
    "contractions_dict = {\n",
    "    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\",\n",
    "    \"doesn’t\": \"does not\", \"don't\": \"do not\", \"don’t\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\", \"he'd\": \"he had\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "    \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y’all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", \"ain’t\": \"am not\", \"aren’t\": \"are not\",\n",
    "    \"can’t\": \"cannot\", \"can’t’ve\": \"cannot have\", \"’cause\": \"because\", \"could’ve\": \"could have\", \"couldn’t\": \"could not\", \"couldn’t’ve\": \"could not have\",\n",
    "    \"didn’t\": \"did not\", \"doesn’t\": \"does not\", \"don’t\": \"do not\", \"don’t\": \"do not\", \"hadn’t\": \"had not\", \"hadn’t’ve\": \"had not have\",\n",
    "    \"hasn’t\": \"has not\", \"haven’t\": \"have not\", \"he’d\": \"he had\", \"he’d’ve\": \"he would have\", \"he’ll\": \"he will\", \"he’ll’ve\": \"he will have\",\n",
    "    \"he’s\": \"he is\", \"how’d\": \"how did\", \"how’d’y\": \"how do you\", \"how’ll\": \"how will\", \"how’s\": \"how is\", \"i’d\": \"i would\", \"i’d’ve\": \"i would have\",\n",
    "    \"i’ll\": \"i will\", \"i’ll’ve\": \"i will have\", \"i’m\": \"i am\", \"i’ve\": \"i have\", \"isn’t\": \"is not\", \"it’d\": \"it would\", \"it’d’ve\": \"it would have\",\n",
    "    \"it’ll\": \"it will\", \"it’ll’ve\": \"it will have\", \"it’s\": \"it is\", \"let’s\": \"let us\", \"ma’am\": \"madam\", \"mayn’t\": \"may not\",\n",
    "    \"might’ve\": \"might have\", \"mightn’t\": \"might not\", \"mightn’t’ve\": \"might not have\", \"must’ve\": \"must have\", \"mustn’t\": \"must not\",\n",
    "    \"mustn’t’ve\": \"must not have\", \"needn’t\": \"need not\", \"needn’t’ve\": \"need not have\", \"o’clock\": \"of the clock\",\n",
    "    \"oughtn’t\": \"ought not\", \"oughtn’t’ve\": \"ought not have\", \"shan’t\": \"shall not\", \"sha’n’t\": \"shall not\", \"shan’t’ve\": \"shall not have\",\n",
    "    \"she’d\": \"she would\", \"she’d’ve\": \"she would have\", \"she’ll\": \"she will\", \"she’ll’ve\": \"she will have\", \"she’s\": \"she is\",\n",
    "    \"should’ve\": \"should have\", \"shouldn’t\": \"should not\", \"shouldn’t’ve\": \"should not have\", \"so’ve\": \"so have\", \"so’s\": \"so is\",\n",
    "    \"that’d\": \"that would\", \"that’d’ve\": \"that would have\", \"that’s\": \"that is\", \"there’d\": \"there would\", \"there’d’ve\": \"there would have\",\n",
    "    \"there’s\": \"there is\", \"they’d\": \"they would\", \"they’d’ve\": \"they would have\", \"they’ll\": \"they will\", \"they’ll’ve\": \"they will have\",\n",
    "    \"they’re\": \"they are\", \"they’ve\": \"they have\", \"to’ve\": \"to have\", \"wasn’t\": \"was not\", \"we’d\": \"we would\", \"we’d’ve\": \"we would have\",\n",
    "    \"we’ll\": \"we will\", \"we’ll’ve\": \"we will have\", \"we’re\": \"we are\", \"we’ve\": \"we have\", \"weren’t\": \"were not\", \"what’ll\": \"what will\",\n",
    "    \"what’ll’ve\": \"what will have\", \"what’re\": \"what are\", \"what’s\": \"what is\", \"what’ve\": \"what have\", \"when’s\": \"when is\",\n",
    "    \"when’ve\": \"when have\", \"where’d\": \"where did\", \"where’s\": \"where is\", \"where’ve\": \"where have\", \"who’ll\": \"who will\",\n",
    "    \"who’ll’ve\": \"who will have\", \"who’s\": \"who is\", \"who’ve\": \"who have\",\"why’s\": \"why is\", \"why’ve\": \"why have\", \"will’ve\": \"will have\",\n",
    "    \"won’t\": \"will not\", \"won’t’ve\": \"will not have\", \"would’ve\": \"would have\", \"wouldn’t\": \"would not\", \"wouldn’t’ve\": \"would not have\",\n",
    "    \"y’all\": \"you all\", \"y’all\": \"you all\", \"y’all’d\": \"you all would\", \"y’all’d’ve\": \"you all would have\", \"y’all’re\": \"you all are\",\n",
    "    \"y’all’ve\": \"you all have\", \"you’d\": \"you would\", \"you’d’ve\": \"you would have\", \"you’ll\": \"you will\", \"you’ll’ve\": \"you will have\",\n",
    "    \"you’re\": \"you are\", \"you’re\": \"you are\", \"you’ve\": \"you have\"\n",
    "}\n",
    "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "# Function expand the contractions if there's any\n",
    "def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, s)\n",
    "\n",
    "\n",
    "# Expanding the contractions\n",
    "data['review_clean'] = data['review_clean'].apply(lambda x: expand_contractions(x))\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4246878-9511-4c53-b506-53d16671d094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Stopwords to extract the useful words in review_clean\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data['review_clean'] = data['review_clean'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44e71e1-012e-47d7-8d0c-9a3aa2235e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the word stems using the Snowball Stemmer in review_clean\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snow_ball_stemmer = SnowballStemmer(language = \"english\")\n",
    "data['review_clean'] = data['review_clean'].apply(lambda x: \" \".join(snow_ball_stemmer.stem(word) for word in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47b4e6e-8d71-4441-92c1-08bfdeb1a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment polarity of all reviews\n",
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment_polarity(review):\n",
    "    pol = []\n",
    "    for i in review:\n",
    "        analysis = TextBlob(i)\n",
    "        pol.append(analysis.sentiment.polarity)\n",
    "    return pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f5ada5-e818-4b1e-b1de-cc1baee8ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Sentiment Polarity for Uncleaned Reviews\n",
    "data['sentiment'] = sentiment_polarity(data['review'])\n",
    "\n",
    "# Evaluating Sentiment Polarity for Cleaned Reviews\n",
    "data['sentiment_clean'] = sentiment_polarity(data['review_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405d326a-3c01-433e-9c2c-affa43bacb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the reviews without using Snowball Stemmer and Stopwords\n",
    "data['review_clean_ss'] = review_clean(data['review'])\n",
    "\n",
    "# Evaluating Sentiment Polarity for cleaned Reviews without using Snowball Stemmer and Stopwords\n",
    "data['sentiment_clean_ss'] = sentiment_polarity(data['review_clean_ss'])\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5ba1f5-9060-4c72-959b-8e07e5acdf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Missing Values\n",
    "data = data.dropna(how=\"any\", axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea116d1-021d-4aa9-ba53-277c4798a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Words count in each review\n",
    "data['words_count']=data[\"review_clean_ss\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Unique words count in each review\n",
    "data['unique_words_count']=data[\"review_clean_ss\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "# Letters count in each review\n",
    "data['letters_count']=data[\"review_clean_ss\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "# Punctuations count in each review\n",
    "data[\"punctuations_count\"] = data[\"review\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "# Upper case words count in each review\n",
    "data[\"uppercase_words_count\"] = data[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "# Title case words count in each review\n",
    "data[\"titlecase_words_count\"] = data[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "# Stopwords count in each review\n",
    "data[\"stopwords_count\"] = data[\"review\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\n",
    "\n",
    "# Average length of the words in each review\n",
    "data[\"average_word_length\"] = data[\"review_clean_ss\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3ef24d-0762-4380-a838-ca4c9220b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711b781-00c2-4cc2-aaf3-6ecf78c26c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding Drugname and Conditions\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder_feat = {}\n",
    "for feature in ['drugName', 'condition']:\n",
    "    label_encoder_feat[feature] = LabelEncoder()\n",
    "    data[feature] = label_encoder_feat[feature].fit_transform(data[feature])\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5134c7-48ca-4e63-b294-e328c0919f1c",
   "metadata": {},
   "source": [
    "### Define Features and Spliting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a8f6f1-f9a8-4b82-b19c-b296ab22b5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data[['condition', 'usefulCount', 'Day', 'Month', 'Year', 'sentiment', 'sentiment_clean_ss', \n",
    "                'words_count', 'unique_words_count', 'letters_count','punctuations_count', 'uppercase_words_count',\n",
    "                'titlecase_words_count', 'stopwords_count', 'average_word_length']]\n",
    "y = data['Review_Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2bcb12-e96a-4d8d-8aaf-5565b837d32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27d74d-5199-4df1-9ced-3d9215bcdf6f",
   "metadata": {},
   "source": [
    "### LGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebda905-fe1b-4d6d-8215-234483939fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "clf = LGBMClassifier(\n",
    "        n_estimators = 10000,\n",
    "        learning_rate = 0.10,\n",
    "        num_leaves = 30,\n",
    "        subsample = .9,\n",
    "        max_depth = 7,\n",
    "        reg_alpha = .1,\n",
    "        reg_lambda = .1,\n",
    "        min_split_gain = .01,\n",
    "        min_child_weight = 2,\n",
    "        silent=-1,\n",
    "        verbose=-1,\n",
    "        )\n",
    "model_one = clf.fit(X_train, y_train)\n",
    "pred_one_test = model_one.predict(X_test)\n",
    "pred_one_train = model_one.predict(X_train)\n",
    "\n",
    "print (\"Accuracy on Training Data: \", accuracy_score(y_train, pred_one_train))\n",
    "print (\"Accuracy on Testing Data: \", accuracy_score(y_test, pred_one_test))\n",
    "print (\"Precision on Testing Data: \", precision_score(y_test, pred_one_test))\n",
    "print (\"Recall on Testing Data: \", recall_score(y_test, pred_one_test))\n",
    "\n",
    "lgbm_clf_train = accuracy_score(y_train, pred_one_train)\n",
    "lgbm_clf_test = accuracy_score(y_test, pred_one_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83712a00-1786-421f-a83f-5f4ff8cabfa4",
   "metadata": {},
   "source": [
    "### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a40adaf-f90b-4837-a0b5-96944c1f9426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "model_two = xgb.fit(X_train, y_train)\n",
    "pred_two_train = model_two.predict(X_train)\n",
    "pred_two_test = model_two.predict(X_test)\n",
    "\n",
    "print (\"Accuracy on Training Data: \", accuracy_score(y_train, pred_two_train))\n",
    "print (\"Accuracy on Testing Data: \", accuracy_score(y_test, pred_two_test))\n",
    "print (\"Precision on Testing Data: \", precision_score(y_test, pred_two_test))\n",
    "print (\"Recall on Testing Data: \", recall_score(y_test, pred_two_test))\n",
    "\n",
    "xgb_clf_train = accuracy_score(y_train, pred_two_train)\n",
    "xgb_clf_test = accuracy_score(y_test, pred_two_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8f0cbc-b0da-4594-a900-02ae88c7cdd0",
   "metadata": {},
   "source": [
    "### CatBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc0c3b-d494-4521-be62-d9ae4f22ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "cat_clf = CatBoostClassifier(iterations = 100, learning_rate = 0.5)\n",
    "\n",
    "model_three = cat_clf.fit(X_train, y_train)\n",
    "pred_train_three = model_three.predict(X_train)\n",
    "pred_test_three = model_three.predict(X_test)\n",
    "\n",
    "print (\"Accuracy on Training Data: \", accuracy_score(y_train, pred_train_three))\n",
    "print (\"Accuracy on Testing Data: \", accuracy_score(y_test, pred_test_three))\n",
    "print (\"Precision on Testing Data: \", precision_score(y_test, pred_test_three))\n",
    "print (\"Recall on Testing Data: \", recall_score(y_test, pred_test_three))\n",
    "\n",
    "catb_clf_train = accuracy_score(y_train, pred_train_three)\n",
    "catb_clf_test = accuracy_score(y_test, pred_test_three)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02adf8d1-3fc2-4564-8a9e-86175d659811",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de452ca-9366-4aec-bcca-c3bc6112cf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators= 5, criterion=\"entropy\")  \n",
    "\n",
    "model_five = rf_clf.fit(X_train, y_train)  \n",
    "pred_train_five = model_five.predict(X_train)\n",
    "pred_test_five = model_five.predict(X_test)\n",
    "\n",
    "print (\"Accuracy on Training Data: \", accuracy_score(y_train, pred_train_five))\n",
    "print (\"Accuracy on Testing Data: \", accuracy_score(y_test, pred_test_five))\n",
    "print (\"Precision on Testing Data: \", precision_score(y_test, pred_test_five))\n",
    "print (\"Recall on Testing Data: \", recall_score(y_test, pred_test_five))\n",
    "\n",
    "rf_train = accuracy_score(y_train, pred_train_five)\n",
    "rf_test = accuracy_score(y_test, pred_test_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03afb142-c60a-4baf-9aa2-8246dfee17e2",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185262d5-b325-4633-b665-eccaf0222084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_clf = GradientBoostingClassifier()\n",
    "\n",
    "model_six = gb_clf.fit(X_train, y_train)  \n",
    "pred_train_six = model_six.predict(X_train)\n",
    "pred_test_six = model_six.predict(X_test)\n",
    "\n",
    "print (\"Accuracy on Training Data: \", accuracy_score(y_train, pred_train_six))\n",
    "print (\"Accuracy on Testing Data: \", accuracy_score(y_test, pred_test_six))\n",
    "print (\"Precision on Testing Data: \", precision_score(y_test, pred_test_six))\n",
    "print (\"Recall on Testing Data: \", recall_score(y_test, pred_test_six))\n",
    "\n",
    "gb_clf_train = accuracy_score(y_train, pred_train_six)\n",
    "gb_clf_test = accuracy_score(y_test, pred_test_six)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f810b6-e431-4180-af3b-2c587ee25467",
   "metadata": {},
   "source": [
    "### Bar Plot of every applied models testing and training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dac383-a30d-4601-944e-cd6633a1638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "barWidth = 0.25\n",
    "fig = plt.subplots(figsize =(14, 8))\n",
    "\n",
    "# set height of bar\n",
    "train_accuracy = [lgbm_clf_train, xgb_clf_train, catb_clf_train, rf_train, gb_clf_train]\n",
    "test_accuracy = [lgbm_clf_test, xgb_clf_test, catb_clf_test, rf_test, gb_clf_test]\n",
    "\n",
    "# Set position of bar on X axis\n",
    "br1 = np.arange(len(test_accuracy))\n",
    "br2 = [x + barWidth for x in br1]\n",
    "\n",
    "# Make the plot\n",
    "plt.bar(br1, test_accuracy, color ='skyblue', width = barWidth, edgecolor ='black', label ='Testing Accuracy')\n",
    "plt.bar(br2, train_accuracy, color ='gray', width = barWidth, edgecolor ='black', label ='Training Accuracy')\n",
    "\n",
    "# Adding Xticks\n",
    "plt.xlabel('Machine Learning Models', fontsize = 15)\n",
    "plt.ylabel('Accuracy', fontsize = 15)\n",
    "plt.xticks([r + barWidth for r in range(len(test_accuracy))],\n",
    "\t\t['LGBM Classifier', 'XGBoost Classifier', 'CatBoost Classifier', 'Random Forest Clf', 'Gradient Bossting Classifier'])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce44539-8d3b-416a-8041-95f9df49c2fd",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da4541d-6434-4565-bdf1-fca7561c5490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a predictive system\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb = LabelEncoder()\n",
    "\n",
    "input_data = (314, 5, 20, 5, 2012, 0.111111, 0.111111, 17, 17, 79, 3, 1, 6,\t7, 3.705882) \n",
    "# input_data = (314, 5, 7, 3, 2017, 0.111111, 0.111111, 30, 27, 136, 11, 2, 4, 8, 3.566667) => Negative\n",
    "# Changing the input_data to numpy array\n",
    "input_data_as_numpy_array = np.asarray(input_data)\n",
    "\n",
    "# Reshaping the array as we are predicting for one instance\n",
    "input_data_reshape = input_data_as_numpy_array.reshape(1,-1)\n",
    "\n",
    "prediction = clf.predict(input_data_reshape)\n",
    "print(prediction)\n",
    "\n",
    "if(prediction[0]==1):\n",
    "    print(\"Positive\")\n",
    "else:\n",
    "    print(\"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4c3427-09ee-4630-af33-99f79e69fe0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
